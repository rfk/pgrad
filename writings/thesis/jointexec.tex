

\chapter{Joint Executions}

\label{ch:jointexec}

This chapter constructs a new data structure for representing the
actions to be performed by a team of agents during the cooperative
execution of a shared task. Dubbed \emph{joint executions}, they are
partially-ordered branching sequences of actions that allow independent
actions to be performed independently, while using each agent's local
view to ensure that synchronisation is always possible when required.

The fundamental unit of reasoning in the situation calculus, and the
output of the ConGolog execution planning process, is the \emph{situation}:
a complete, ordered history of all actions that are to be performed.
This is suboptimal for representing plans in an asyncrhonous multi-agent
setting in three distinct ways:

\begin{itemize}
\item it does not permit branching depending on information obtained at
run-time 
\item it enforces a strict execution order on actions that are potentially
independent, requiring inter-agent syncrhonisation when it is not
actually necessary 
\item it demands a strict execution order on actions that may be hidden
from some agents, demanding inter-agent synchronisation that is not
actually possible 
\end{itemize}
As we have demonstrated in Chapter \ref{ch:mindigolog}, restricting
the domain to be synchronous and completely known lets the agents
make effective use of raw situation terms for planning. But moving
to asynchronous domains with incomplete knowledge requires a more
powerful structure for representing the actions to be performed.

To build such a structure, we take inspiration from a model of concurrent
computation known as \emph{prime event} \emph{structures} \citep{npw79event_structures},
which are partially-ordered branching sequences of events. A \emph{joint
execution} is defined as a particular kind of prime event structure
that is rich enough to capture the concurrent execution of independent
actions, including actions with sensing results, but restricted enough
that it can be unambiguously reduced back to ordinary situation terms
for the purposes of reasoning. Using our explicit account of the local
\emph{view} of each agent from Chapter \ref{ch:observations}, we
identify several restrictions on a joint execution that make it a
feasible to execute in the real world.

Joint executions thus allow us to capture the actions that a team
of agents are to perform in service of some shared task, without requiring
constant synchronization between the agents, and without assuming
that agents know all the actions that have been performed, while utilizing
existing reasoning methods based on full situation terms. This is
a significant increase in expressiveness over existing approaches
to modeling multi-agent teams in the situation calculus. To demonstrate
the utility of the approach, we extend our MIndiGolog interpreter
from Chapter \ref{ch:mindigolog} to show how a team of agents can
cooperate to plan and perform the execution of a shared ConGolog program
in an asynchronous, partially observable domain.

The chapter proceeds as follows: TODO.


\section{Background\label{sec:JointExec:Background}}

The above discussion highlights three desirable properties of a plan
representation formalism intended for use in rich multi-agent domains:
it must be \emph{branching}, \emph{partially-ordered}, and \emph{feasible}.
While each of these aspects have been studied in isolation in this
situation calculus, our work is the first to combine them into a single
formalism suitable for a multi-agent setting.


\subsection{Branching}

Several single-agent formalisms based on the situation calculus have
introduced some form of branching into the structures returned by
the planner, including the conditional action trees of \citep{lakemeyer99golog_cats}
and the branching plans of \citep{giacomo04sem_delib_indigolog}.
These structures typically branch based on the truth or falsity of
various test conditions included in the program, rather than directly
on the sensing results returned by an action. For example, the definition
of conditional action trees in \citep{lakemeyer99golog_cats} includes
the following branching case:\[
c=[\phi,c_{1},c_{2}]\]


This instructs the agent to execute the sub-tree $c_{1}$ if $\phi$
is true and the sub-tree $c_{2}$ if $\phi$ is false. While this
works well in a single-agent setting, it is not clear how to extend
it to a multi-agent setting, where some members of the team may not
know whether or not $\phi$ holds. Intsead, we will construct plans
that branch directly on the results returned by sensing actions, in
the style of the {}``robot programs'' of \citep{levesque96what_is_planning}.


\subsection{Partial Order}

There has been little work on partial-order planning in the situation
calculus, most likely because the use of situations heavily biases
the reasoning machinery towards totally-ordered sequences of actions.
One exception is \citep{plaisted97sc_aspect}, which extends the situation
calculus with explicit {}``aspects'' and allows partial-ordering
between actions that affect different aspects of the world state.

Partial-order planning is the mainstay of the closely-related formalism
of the \emph{event calculus}. Here action occurrences are specified
by asserting that they happen at a particular time, and the constraints
on relative occurrence times of actions determine a partial ordering.
\citet{Shanahan97ec_planning} has shown that abductive theorem proving
in the event calculate generates partially-ordered plans, and the
mechanics of the theorem prover naturally mirror various concepts
from the literature on explicit partial-order planning, such as threats,
links and conflicts \citep{peot92conditional_nonlinear}.

The close similarities between the situation and event calculi are
well understood, as are the advantages of the event calculus when
it comes to working with partially-ordered action sequences \citep{belleghem97sitcalc_evtcalc}.
Indeed, it is possible to implement a Golog interpreter on top of
the event calculus that naturally generates partially-ordered plans
\citep{pereira04ec_golog}. Perhaps we should adopt such a formalism
that is naturally partially-ordered, rather than trying to construct
partial orders on top of the naturally sequential situation calculus?

While a partial-order representation is important, as we don't want
the agents to have to synchronise their actions unnecessarily, it
is not the complete picture. We also need the converse: that when
an explicit ordering between actions is necessary, the required synchronisation
is possible. It is not clear how techniques such as \citep{pereira04ec_golog}
would extend to the multi-agent case.

As we shall see, the formalisms we develop in this thesis enable these
dual requirements - that some actions don't need to be ordered, while
other actions cannot be ordered - to be captured in the situation
calculus in quite an elegant way.


\subsection{Feasibility}

To allow an agent to execute a plan that depends on information collected
at run-time, it is not sufficient to simply introduce branching into
the plan representation formalism. One must also ensure that, at execution
time, the agent will always \emph{know} which branch of the plan to
take. For example, suppose this simple branching plan will provably
achieve a goal:\[
\mathbf{if}\,\phi\,\mathbf{then}\, action_{1}\,\mathbf{else}\, action_{2}\]


The agent can only execute this program if it know whether or not
$\phi$ holds -- otherwise, although one of the branches is guaranteed
to achieve the goal, the agent does not know which one to take. This
requirement that an agent {}``knows how'' to execute a plan is formalised
by various notions of \emph{epistemic feasibility} including those
of \citep{levesque98what_robots_can_do,levesque00knowing_how,Lesperance01epi_feas_casl,giacomo04sem_delib_indigolog,baier06programs_that_sense}.

One approach to ensuring feasibility, embodied by \citep{levesque00knowing_how,giacomo04sem_delib_indigolog,baier06programs_that_sense},
is to allow a plan to be potentially any program formulated in a control
language such as Golog. They then semantically characterise those
programs that are epistemically feasible, using direct assertions
about the knowledge of each agent at each stage of execution. While
this allows for potentially very succinct plans, it is not clear how
to systematically generate an epistmically feasible plan characterised
in this very general way.

Another approach, advocated by \citep{levesque96what_is_planning,levesque98what_robots_can_do}
and used in the implementation section of \citep{giacomo04sem_delib_indigolog},
is to restrict the structure of plans so that they are obviously epistemically
feasible. For example, the {}``robot programs'' of \citep{levesque98what_robots_can_do}
are restricted to simple operators such as:\begin{gather*}
action\\
seq(\delta_{1},\delta_{2})\\
branch(action,\delta_{1},\delta_{2})\\
loop(branch(action,\delta,exit))\end{gather*}


These programs do not contain test conditions, but rather branch directly
on the (binary) sensing results returned from each action. There is
therefore no potential for confusion when executing such programs,
as they are essentially equivalent to a kind of finite automata that
can be executed reactively. Nevertheless, \citet{levesque98what_robots_can_do}
show that these programs are universal, in the sense that any achievable
goal can be achieved by suitable a robot program. We are not aware
of any work extending this approach to represent programs to be exeuted
cooperatively by a team of agents.

This notion of epistemic feasibility can be characterised as \emph{knowing
what} -- at each stage of execution, each agent must know what its
next action is. In synchronous domains with public actions, as typically
studied in the situation calculus, this is all that is necessary to
ensure the executability of a plan.

In asynchronous domains it is not enough for an agent to know \emph{what}
its next action is. It must also know \emph{when} that action should
be performed. For example, suppose that the following simple plan
provable achieves a goal:\[
action_{1}(agt_{1})\,;\, action_{2}(agt_{2})\]


In a synchrononous domain this plan can be executed directly. But
suppose the domain is asynchronous, and $agt_{2}$ is unable to observe
the occurrence of $action_{1}$. Since $agt_{2}$ has no way of knowing
whether or not $action_{1}$ has been performed, it will not know
when to perform $action_{2}$ and the plan cannot be executed.

In this chapter we adopt an approach similar to \citep{levesque98what_robots_can_do},
and define a structure for representing plans that forces all plans
to be epistemically feasible. We use the explicit account of each
agent's local view developed in the previous chapter to ensure that
each agent will know when to perform its action.


\subsection{Event Structures}

To tackle cooperative execution in a multi-agent setting, we have
adopted a model of concurrent computation known as \emph{event structures}
\citep{npw79event_structures}. The particular variant we are interested
in are \emph{prime event structures}, canonically defined as a four-tuple:\[
(\mathcal{V},\prec,\#,\gamma)\]
 Here $\mathcal{V}$ is a set of events; $\gamma$ is a function assigning
a label to each event; $\prec$ is the \emph{enabling} relation, a
strict partial order on events; and $\#$ is the \emph{conflict} relation,
a binary symmetric relation that is inherited through enablers:\[
\forall i,j,k\in V:i\#j\wedge i\prec k\rightarrow k\#j\]


A \emph{configuration} is a sequence of events consistent with $\prec$
in which no pair of events conflict. Each configuration thus represents
a potential partial run of execution of the system. These structures
can thus be seen as partially-ordered branching sequences of events.
For convenience, we define our prime event structures in a slightly
different form:\[
(\mathcal{V},ens,alts,\gamma)\]


Here $ens(i)$ assigns to each event $i\in\mathcal{V}$ a finite set
of other events that are its direct \emph{enablers}, meaning that
event $i$ can only occur after all the events in $ens(i)$ have occurred.
Similarly $alts(i)$ assigns to each event a set of direct \emph{alternatives},
meaning that only one of the events in $\{i\}\cup alts(i)$ can occur
during execution of the system. The enabling relation $i\prec j$
is then the transitive closure of $i\in ens(j)$, and $\#$ is the
smallest relation satisfying:\[
\forall i,j\in V:[\exists i',j'\in V:i'\preceq i\wedge j'\preceq j\wedge i\in alts(j)]\rightarrow i\#j\]
 Considered in this way, event structures form a directed acyclic
graph of the events that could occur during execution of the system.
As shown in \citep{pratt91modeling_conc_with_geom}, it is straightforward
to convert these structures into a kind of finite automaton for efficient
execution, much like the robot programs of \citep{levesque98what_robots_can_do}.


\section{Joint Executions\label{sec:JointExec:JEs}}

We define a \emph{joint execution} as a special kind of prime event
structure with the following components:\[
(\mathcal{A},\mathcal{O},ens,alts,\gamma,<)\]
 It contains two disjoint sets of events: \emph{action} events $\mathcal{A}$
representing the actions to be performed, and \emph{outcome} events
$\mathcal{O}$ representing the possible outcomes of each action.
For each action event $a\in\mathcal{A}$, its enablers $ens(a)$ is
a set of outcome events, its alternatives $alts(a)$ is empty, and
its label $\gamma(a)$ is the action to be performed. For each outcome
event $o\in\mathcal{O}$, $ens(o)$ is a single action event for which
it is a possible outcome, $alts(o)$ is the set of all other outcome
events $o'$ such that $ens(o')=ens(o)$, and $\gamma(o)$ is an outcome
as produced by the $Out(a,s)$ function for the action $\gamma(ens(o))$.

A simple example of a joint execution is shown in Figure \ref{fig:example-je}.
Here action $act1$ has two possible outcomes, which enable different
actions $act2$ and $act3$.

%
\begin{figure}
\framebox{%
\begin{minipage}[t][1\totalheight]{1\columnwidth}%
\textsf{\textbf{\tiny \input{listings/jointexec/je_example.tex}}}{\tiny {} }%
\end{minipage}}

\caption{ A simple joint execution. Elliptical nodes are action events, box
nodes are outcome events. }


\label{fig:example-je} 
\end{figure}


A joint execution has one additional component over a standard prime
event structure: a \emph{total} order on events $<$ that is consistent
with the partial order $\prec$ induced by the enabling relation.
We will use this to perform reasoning by assuming that events occur
in the fixed order given by $<$, and placing restrictions on the
joint execution to ensure this assumption is acceptable. In practice,
$<$ will be determined by the order of insertion of events.

An \emph{outcome set} is a minimal set of non-conflicting outcome
events; that is, a set of events $e\subset\mathcal{O}$ satisfying:\begin{gather*}
\forall o_{1},o_{2}\in e\,:\,\neg(o_{1}\#o_{2})\\
\forall o_{1},o_{2}\in e\,:\, o_{1}\not\prec o_{2}\,\wedge o_{2}\not\prec o_{1}\end{gather*}


Recall that a \emph{configuration} is a partial run of execution of
the prime event structure. Clearly any configuration ending in an
outcome event corresponds to a unique history, as it is a set of alternating
actions and their outcomes. We call the \emph{histories} of an outcome
set, denoted $hists(e)$, the set of all configurations that contain
all elements of $e$, and end in an event from $e$. The unique history
of an outcome set that is consistent with $<$ will be denoted $hist(e)$.

We say that an outcome set $e_{1}$ \emph{covers} a set \emph{$e_{2}$,}
denoted by $e_{1}\sqsubseteq e_{2}$, if every event in $e_{1}$ is
either also in $e_{2}$, or precedes something in $e_{2}$. Every
history in $hists(e_{2})$ will have a prefix in $hists(e_{1})$.
This defines a partial ordering on outcome sets:\[
e_{1}\sqsubseteq e_{2}\,\equiv\,\forall o_{1}\in e_{1}:\,\,\exists o_{2}\in e_{2}:\,\, o_{1}\preceq o_{2}\]


Let $max(e)$ denote the maximal element of an outcome set with respect
to the total order $<$. A \emph{branch}, denoted $b$, is a special
case of an outcome set that meets the following additional requirement:\[
\forall i<max(b):\,\, i\in b\,\oplus\,(\exists i'\in b:\,\, i\#i'\,\vee\, i\prec i')\]


That is, every event less than the maximal element of $b$ is either
in the branch, conflicts with something in the branch, or precedes
something in the branch. A branch thus identifies a unique outcome
for each action event that is ordered below $max(b)$.


\section{Independent Actions\label{sec:JointExec:IndepActs}}

As discussed in Section \ref{sec:Background}, posing queries in the
situation calculus requires a full situation term, which means a total
order on all actions that have occurred. The first step towards providing
only a \emph{partial} order on the actions to be performed is, therefore,
to capture the conditions under which actions can be performed out
of order without invalidating the results of the reasoning process.

Define \emph{independent} actions, identified by $indep(a_{1},a_{2})$,
as those that can be performed in either order without affecting what
holds in the resulting situation. Formally, they must satisfy the
following restrictions (where $\mathcal{P}$ and $\mathcal{F}$ are
meta-variables ranging over predicate and functional fluents respectively):

\begin{itemize}
\item $Poss(a_{1},s)\equiv Poss(a_{1},do(a_{2},s))$ 
\item $Poss(a_{2},s)\equiv Poss(a_{2},do(a_{1}s))$ 
\item $\mathcal{P}(do(a_{1},do(a_{2},s)))\equiv\mathcal{P}(do(a_{2},do(a_{1},s)))$\\
 for all predicate fluents $\mathcal{P}(s)$ 
\item $\mathcal{F}(do(a_{1},do(a_{2},s)))=r\equiv\mathcal{F}(do(a_{2},do(a_{1},s)))=r$\\
 for all functional fluents $\mathcal{F}(s)$ 
\end{itemize}
Whether actions are independent can be deduced from the description
of the action theory, or (as currently in our implementation) indicated
explicitly by the programmer.

We will say that two situations are \emph{equivalent} if they are
identical up to transposition of pairs of adjacent independent actions.
A straightforward case analysis on the definition of the regression
operator shows that for equivalent situations $s_{1}$ and $s_{2}$,
$\Dt\models\phi(s_{1})$ iff $\Dt\models\phi(s_{2})$.

This notion of equivalence can be extended by macro expansion to histories
in the obvious manner, with the analogous result that $\Dt\cup\mathbf{sensed}[\sigma_{1}]\models\phi(\mathbf{end}[\sigma_{1}])$
iff $\Dt\cup\mathbf{sensed}[\sigma_{2}]\models\phi(\mathbf{end}[\sigma_{2}])$.


\section{Legal Joint Executions\label{sec:JointExec:Legal}}

We now impose several restrictions on the structure of a joint execution,
to ensure they are suitable for representing the actions to be performed
by a team of agents.\\


\textbf{(R1) Independent events have independent actions:}\\
 Joint executions are restricted such that the following holds for
all action events $a_{1},a_{2}\in\mathcal{A}$:\[
a_{1}<a_{2}\,\rightarrow\, a_{1}\prec a_{2}\,\vee\, a_{1}\#a_{2}\,\vee\, indep(\gamma(a_{1}),\gamma(a_{2}))\]


In words: if event $a_{2}$ is after $a_{1}$ in the total order,
but the execution allows them to be performed in either order, then
the corresponding actions must be independent.

With this restriction, the histories given by $hists(b)$ will differ
from each other only by transposition of independent actions, making
them equivalent to the unique history $hist(b)$. It is thus possible
to determine whether a formula holds after the execution of a given
branch, using the query:\[
\Dt\cup\mathbf{sensed}[hist(b)]\models\phi(\mathbf{end}[hist(b)])\]


This is a key point and is worth re-iterating: regardless of the precise
order in which events are executed, any history that contains all
events on a branch $b$, and that ends in an event from $b$, is equivalent
to the unique history of $b$ generated by the total order. This allows
us to plan using execution branches rather than histories or situation
terms.\\


\textbf{(R2) Actions are enabled by a unique branch:}\\
 We restrict the enabling set for each action event to be a branch,
rather than an arbitrary set of outcome events. In practice this means
that when inserting a new action into the joint execution, one must
specify the branch to which it belongs. This restriction allows us
to determine what holds immediately before an event $a\in\mathcal{A}$
becomes enabled, using $hist(ens(a))$. Given that the action $\gamma(a)$
is possible after $ens(a)$, then when $a$ becomes enabled it will
remain possible until it is performed. Moreover, any events that occur
after it is enabled but before it is performed will not interfere
with its effects, as these actions must be independent of $\gamma(a)$.
\\


\textbf{(R3) All possible outcomes are considered:}\\
 Clearly planning requires that all possible outcomes of an action
be considered. For each action event $a\in\mathcal{A}$, and each
possible outcome $r$ of that action, if:\begin{multline*}
D\cup\mathbf{sensed}[hist(ens(a))]\not\,\models\,\\
Out(\gamma(a),\mathbf{end}[hist(ens(a))])\neq r\end{multline*}


then there must be a corresponding outcome event:\[
\exists o\in E:\,\, ens(o)=\{a\}\,\wedge\,\gamma(o)=r\]


\textbf{(R4) Actions are enabled by observable events:}\\
 If an action is to be enabled by an outcome event $o$ produced by
another agent, it is clearly necessary that the agent performing the
action be able to observe the occurrence of $o$. Otherwise, it has
no way of synchronizing its actions with those of its teammate. Let
$actor(a)$ be the agent responsible for performing an action event
$a$, then we require that:\[
\forall o\in ens(a):\,\,\gamma(o)[actor(a)]\neq nil\]


\textbf{(R5) Overlapping views enable identical actions:}\\
 To ensure that the joint execution can actually be carried out by
the agents, there must be no confusion about whether a particular
action is enabled. Lifting the function $view$ to operate on sets
of histories in the obvious way, then:\[
view(actor(a),\, hists(ens(a)))\]
 gives the set of all local histories after which $actor(a)$ is required
to perform the action $\gamma(a)$. However, since the agent has only
a local viewpoint, it may be the case that some other outcome set
can produce an identical local history.

Say that two branches overlap, denoted $overlaps(b,b')$, if they
could produce an identical local history from the perspective of a
given agent:\[
view(agt,hists(b))\,\cap\, view(agt,hists(b'))\,\neq\varnothing\]


Then the \emph{minimal overlapping set} for $b$, from the perspective
of $agt$, is the set of all $b'$ satisfying:\[
overlaps(agt,b,b')\wedge\neg\exists b''\left[overlaps(agt,b,b'')\wedge b''\sqsubset b'\right]\]


This set captures all branches that the agent could potentially confuse
for $b$. To ensure there is no confusion about whether an action
is enabled, for each $a\in\mathcal{A}$, every $b$ in the minimal
overlapping set of $ens(a)$ for $actor(a)$ must enable an event
$a'$ with identical action $\gamma(a')=\gamma(a)$. This ensures
that the agent's local information is always enough to know when it
should perform an action. While it may not know precisely which \emph{event}
is enabled, it will know enough to determine the specific \emph{action}
that it must perform.


\section{Planning with Joint Executions\label{sec:JointExec:Planning}}

Our implementation of an execution planning system uses joint executions
as an abstract data type that can be built up one event at a time,
one branch at a time. For a particular branch $b$, the state of the
world is reasoned about using standard regression techniques over
$hist(b)$, to determine the next action to perform. This action is
then be inserted into the joint execution to extend the branch.

Inserting a new action event requires specifying the action to be
performed, the branch on which to insert it, and a set of existing
events on that branch that must precede it. The code managing the
joint execution determines all possible outcomes of the action and
adds them as outcome events, returning a new branch for each outcome.
It also ensures that the restrictions in Section \ref{sub:Restrictions}
are satisfied, which can involve forcing an ordering between potentially
concurrent events to ensure independence (R1) or synchronization (R4),
or adding actions to other branches that the acting agent cannot distinguish
from the current one (R5). If the restrictions cannot be met, insertion
fails and the planner must backtrack to find a different action.

The intricacies of synchronization between agents in the face of partial
observability are thus hidden from the planning algorithm itself,
automatically managed by the construction of a joint execution.


\section{Summary\label{sec:JointExec:Summary}}

In this section we have defined a \emph{joint execution} as a prime
event structure with some additional restrictions. We contend that
such structures are highly suitable for planning the actions to be
performed by a team in service of some shared task, such as executing
a shared ConGolog program.

On one hand, joint executions are restricted enough to be practical
for such use. Like the situation terms or conditional action trees
used in previous approaches, prime event structures are purely reactive
(equivalent to a kind of finite automaton) and can be executed without
further deliberation. They are restricted to ensure that whenever
an agent is required to perform an action, it is able to determine
this using only its local information. Each branch of execution can
be easily converted into a situation term for the purposes of reasoning,
and can be extended one action at a time.

Joint executions are also significantly more flexible than previous
approaches. They allow independent actions to be performed without
synchronization, in any order. The agents need never know precisely
what actions have been executed, only those that enable them to perform
their next action. Synchronization is automatically achieved when
required by explicitly reasoning about what actions each agent can
observe, rather than requiring that all actions be public.

To demonstrate the utility of these structures, we have implemented
an interpreter for multi-agent ConGolog programs that produces joint
executions as its output. In the next section, we highlight the key
aspects of our implementation and give an example of the output it
produces.


\section{Implementation\label{sec:JointExec:Implementation}}

Our implementation of a ConGolog execution planner utilizes the Mozart
programming system, which implements the Oz language \citep{vanroy99mozart}
rather than the traditional use of Prolog. As we shall see, Mozart's
strong support for distributed logic programming allows the team of
agents to share the planning workload with almost no additional code.

To make things more concrete, Figure \ref{fig:plan-output} shows
the output of our system when run on the $MakeSalad$ example of Figure
\ref{fig:makesalad-program}. Since all actions in this program have
a single outcome, the outcome events have been suppressed for brevity.

In this domain there are three agents, but only two knives are available.
The agents must therefore synchronize their use of these resources.
Actions are taken to be independent if they deal with different objects.
As seen in Figure \ref{fig:plan-output}, the use of a partial order
structure facilitates parallelism between the agents, with each processing
a different ingredient and only synchronizing on the availability
of the required resources. There is no need for processing actions,
such as $mix$ and $chop$, to be publicly observable. This execution
is maximally concurrent given the constraints of the domain, and is
clearly a significant improvement over totally ordered sequences of
actions as produced by existing systems.

In the following subsections, we briefly highlight some key aspects
of our implementation.


\subsection{Program Steps}

While the ability to determine whether actions are independent is
necessary in constructing partially-ordered executions of a ConGolog
program, it is not sufficient. Actions might have an order imposed
on them directly by the program (for example by the sequence construct
$a_{1};a_{2}$). The program may require that some additional conditions
are true immediately prior to executing an action (for example to
satisfy a test construct $\phi?$), which could be falsified by an
otherwise independent action.

To ensure that the dependencies between actions reflect the needs
of the program being executed, we augment our implementation of the
$Trans$ predicate to keep additional information about what transitions
were made. A \emph{step} object has the following attributes:

\begin{itemize}
\item action: the action performed in that step, or $nil$ if it is an internal
program transition 
\item test: an additional fluent formula that must hold immediately before
performing the step 
\item thread: a sequence of 'l' and 'r' characters indicating the concurrent
thread in which the step is performed 
\item outcome: the outcome of performing the action. 
\end{itemize}
%
\begin{figure}
\framebox{%
\begin{minipage}[t][1\totalheight]{1\columnwidth}%
\textsf{\textbf{\tiny \input{listings/jointexec/plan.tex}}}%
\end{minipage}}

\caption{ Joint execution for $MakeSalad(bowl(1))$, showing significant concurrency
between agents }


\label{fig:plan-output} 
\end{figure}


We call a sequence of such steps a \emph{run}, which can be converted
to a history by taking just the action and outcome attributes. The
procedure implementing $Trans$ takes a program and a run as input,
returning a new program and new step of execution. As an example consider
the code in Figure \ref{fig:trans-code}, implementing the test operator
and the concurrency operator from equation \ref{eqn:trans_conc_orig}.

%
\begin{figure}
\framebox{%
\begin{minipage}[t][1\totalheight]{1\columnwidth}%
{\scriptsize \verbatiminput{listings/jointexec/ConGolog.oz}}%
\end{minipage}}

\caption{ Partial code for $Trans$ predicate }


\label{fig:trans-code} 
\end{figure}


Note that whenever the procedure descends through the left side of
a concurrency operator it pushes an 'l' onto the step's {}``thread''
attribute, and each descent through the right side pushes an 'r'.
Two steps can be said to come from different threads as long as neither
{}``thread'' attribute is a prefix of the other.

We say that two steps are \emph{ordered} if any of the following holds:
their action terms are not independent; ones thread is a prefix of
the other; ones action falsifies the test condition associated with
the other. When building a joint execution, ordered steps are forced
to be executed in the order they were generated by the planner, while
unordered steps may be performed independently.


\subsection{Planning Procedure}

The code for planning a joint execution from a given ConGolog program
is shown in Figure \ref{fig:planning-code}. The main procedure is
$MakePlan$, a recursive procedure that operates on a list of branches-in-progress
of the form $(D,R,B)$. Here $B$ is a branch in the joint execution
under construction, $D$ is the program remaining to be executed on
that branch, and $R$ is the run of program steps performed on that
branch so far.

%
\begin{figure}
\framebox{%
\begin{minipage}[t][1\totalheight]{1\columnwidth}%
{\scriptsize \verbatiminput{listings/jointexec/Planner.oz}}%
\end{minipage}}

\caption{ Code for main planning loop }


\label{fig:planning-code} 
\end{figure}


%
\begin{figure}
\framebox{%
\begin{minipage}[t][1\totalheight]{1\columnwidth}%
{\scriptsize \verbatiminput{listings/jointexec/psearch.oz}}%
\end{minipage}}

\caption{ Code to run planning procedure in parallel }


\label{fig:parallel-search} 
\end{figure}


Each iteration of the planning loop proceeds as follows. The procedure
$FindOpenBranch$ updates each branch to account for events that were
added since it was last processed (some may have been added automatically
to satisfy restriction (R5)), then searches the list to find a branch
for which $Final(D,R)$ does not hold. If all branches are final,
planning can terminate. Otherwise, the procedure $FindTrans1$ is
called to find a new step of execution for that branch. The action
is inserted into the joint execution, which returns a list of new
branches, one for each possible outcome of the action. Each of these
outcomes is added to the list of branches, and the loop is started
again.

Of particular interest is the procedure $FindTrans1$, which uses
the encapsulated search functionality of Mozart to yield possible
next steps according to an estimate of their potential for concurrency.
The procedure $LP.yieldOrdered$ yields the solutions of the given
search context, sorted using the procedure $CompareSteps$. This procedure
(not shown) gives preference to steps that can be performed concurrently
with as many existing actions as possible.


\subsection{Distributing the Planning Workload}

A primary motivation in using Mozart/Oz for our implementation is
its strong support for distributed logic programming. Utilizing Mozart's
parallel search functionality \citep{Schulte00constraint_services},
the planning workload can be transparently distributed between the
agents in the team.

Figure \ref{fig:parallel-search} shows the necessary code, which
should be executed by one of the agents. The planning procedure is
encapsulated in a \emph{functor}, a portable code object. A parallel
search object is then created, which uses an ssh connection to spawn
remote computations on each of the three agents (identified by their
DNS names). The object is asked to provide a single solution, which
is then written to a file in the graphviz 'dot' format for display
(resulting in Figure \ref{fig:plan-output}).

For the simple example shown in this chapter, parallel plan search
does not demonstrate a significant time saving since almost no backtracking
is required to reach a solution. For more difficult problems, significant
gains can be expected.


\section{Discussion\label{sec:JointExec:Discussion}}

The idea of having a shared ConGolog program that is executed by a
team of agents is used with considerable success in \citep{Ferrein2005readylog},
where the variant Readylog is used to coordinate and control a RoboCup
soccer team. Their domain satisfies the assumption that all actions
are public, so they can afford to produce totally ordered plans. The
techniques developed in this chapter would generalize such an approach
to a wider variety of domains (although \citep{Ferrein2005readylog}
includes some decision theoretic aspects that are not present in our
work).

There has also been significant work using the situation calculus
in settings where each agent has its own control program \citep{shapiro02casl}.
The behavior of the overall system is specified as the concurrent
combination of each agent's program, $\delta_{1}||\delta_{2}||\dots$,
and ConGolog is used to verify that the system satisfies certain constraints
(e.g. safety or liveness properties). Again, this cannot handle partial
observability of actions since all reasoning is based on a full situation
term. We believe the joint execution formalism could be helpful in
such settings, by permitting synchronization only when actions are
observable by the agents.

An alternate approach to the problem of partial observability is the
language TeamGolog developed in \citep{farinelli07team_golog}, where
agents explicitly synchronize through communication and a shared state.
By contrast, our approach constructs synchronization implicitly by
reasoning about the actions that can be observed by each agent. This
has the advantage of requiring no changes to the form or semantics
of the agents' control program, but the disadvantage that joint execution
construction may fail if too many actions are unobservable. It would
be interesting to combine these approaches by automatically incorporating
explicit communication when implicit synchronization is not possible.

Several authors have introduced operators for expressing partial orderings
between actions into the ConGolog language itself, for example \citep{son00htn_golog}.
While this is unquestionably useful for specifying behavior, the \emph{output}
from such systems is still a totally ordered sequence of actions and
thus maintains the limitations of existing systems when actually performing
the execution in a multi-agent setting.

There is, of course, an extensive body of work on partial-order planning
in the context of goal-based planning. Unsurprisingly, the joint execution
structure we develop here has deep similarities to the structures
used in conditional partial-order planners such as \citep{peot92conditional_nonlinear}.
It is, however, intentionally specific to the situation calculus.
We make no use of many concepts common in partial-order goal-based
planning (causal links, threats, conflicts, etc) because we do not
deal explicitly with goals, but with steps generated by an underlying
transition semantics. Our approach can be considered roughly equivalent
to \emph{deordering} of a totally-ordered plan as described in \citep{backstrom99reordering},
except performed during plan construction rather than as a post-processing
step.

loops: \citep{levesque96what_is_planning,levesque05planning_with_loops}

